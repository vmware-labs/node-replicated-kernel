// Copyright © 2021 VMware, Inc. All Rights Reserved.
// Copyright © 2022 The University of British Columbia. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0 OR MIT

//! Kernel TLS variant 2 implementation (for static linking only). Ensures
//! #[thread_local] stuff works after we call `ThreadControlBlock::init()`.

use alloc::alloc::Layout;
use core::mem;
use core::ptr;

use bootloader_shared::TlsInfo;
use kpi::KERNEL_BASE;

use cortex_a::{asm::barrier, registers::*};
use tock_registers::interfaces::{Readable, Writeable};

use crate::error::{KError, KResult};

fn get_tls_region(info: &TlsInfo) -> (&'static [u8], Layout) {
    let tls_layout = Layout::from_size_align(info.tls_len_total as usize, info.alignment as usize)
        .expect("TLS size / alignment issue during init?");

    // Safety `from_raw_parts`:
    // - We know this exists/is valid because our ELF loader put the TLS section
    //   there (hopefully)
    // - memory range of this slice must be contained within a single allocate
    //   object: static blob, put there by the ELF loader, no deallocation
    // - Alignment is `1` for u8 slices:
    static_assertions::const_assert!(mem::align_of::<[u8; 1]>() == 1);
    // - Properly initialized values: It's plain-old-data
    // - total size len * mem::size_of::<T>() of the slice must be no larger than isize::MAX:
    assert!(info.tls_data_len < isize::MAX.try_into().unwrap());
    let tls_region = unsafe {
        core::slice::from_raw_parts(info.tls_data as *const u8, info.tls_data_len as usize)
    };

    (tls_region, tls_layout)
}

/// Per-core TLS state in the kernel.
///
/// - This is what the `fs` register ends up pointing to. Ideally, we could tell
///   rustc to use the `gs` register for eventual perf. benefits, but currently
///   this isn't supported (see also
///   https://github.com/rust-lang/rust/issues/29594#issuecomment-843934159).
///
///
/// - The thread-local-storage variables are stored *in front* of that structure
/// (TLS variant 2 layout).
///
/// - This struct is `repr(C)` because TLS spec depends on the order of the
/// first two elements.
#[repr(C)]
pub(crate) struct ThreadControlBlock {
    /// Points to `self` (makes sure mov %fs:0x0 works)
    ///
    /// This is how the compiler looks up TLS things.
    tcb_myself: *mut ThreadControlBlock,

    /// For dynamic loading (unused but added for future compatibility (we don't
    /// do dynamic linking/modules)).
    tcb_dtv: *const *const u8,
}

impl ThreadControlBlock {
    /// Initialize TLS for the current core.
    ///
    /// # Returns
    /// The memory location that `fs` points to. This is used by the kernel to
    /// restore the register to the correct kernel value on entry.
    ///
    /// # Safety
    /// - `enable_fsgsbase` has already been called on the core (sets bit in
    ///   Cr4)
    /// - Assume that BIOS/UEFI initializes fs with 0x0
    ///   Cr4)
    pub(super) unsafe fn init(info: &TlsInfo) -> KResult<*const ThreadControlBlock> {
        log::info!("initializing TLS with {:?}", info);
        let tcb = ThreadControlBlock::new(info);
        match ThreadControlBlock::try_get_tcb() {
            Some(x) => {
                if (x as *const _ as u64) < KERNEL_BASE {
                    log::warn!("tls already initialized: {:p}, re-initialize", x);
                    ThreadControlBlock::install(tcb);
                    Ok(tcb)
                } else {
                    Err(KError::TLSAlreadyInitialized)
                }
            }
            None => {
                ThreadControlBlock::install(tcb);
                Ok(tcb)
            }
        }
    }


    /// Creates a new thread local storage area.
    ///
    /// Note that the returned pointer points somewhere in the middle/end of the
    /// allocated memory region as the TLS variant 2 layout says the
    /// thread-local data comes *before* the TLS control block.
    ///
    /// # Notes
    ///
    /// Does a bunch of unsafe memory operations to lay out the TLS area.
    ///
    /// Currently leaks the memory, someone else needs to ensure that the
    /// allocated memory is `freed` at some point again if we ever shutdown
    /// cores.
    fn new(info: &TlsInfo) -> *const ThreadControlBlock {
        const TCB_INITIAL: ThreadControlBlock = ThreadControlBlock {
            tcb_myself: ptr::null_mut(),
            tcb_dtv: ptr::null(),
        };

        let (initial_tdata, tls_data_layout) = get_tls_region(info);

        // Allocate memory for a TLS block: variant 2: [tdata, tbss, TCB], and
        // start of TCB goes in fs):

        // Safety `alloc_zeroed`:
        let (tls_all_layout, _offset) = tls_data_layout
            .extend(Layout::new::<ThreadControlBlock>())
            .expect("Can't append ThreadControlBlock to TLS layout during init");
        // Make sure `extend` didn't end up padding for `ThreadControlBlock` (so
        // tdata/tbss is still properly aligned on access): If not (I think) we
        // might lose alignment of `tls_data_layout` because we start from the
        // TCB going backwards. The ELF Handling For TLS by Drepper
        // unfortunately does not talk a lot about alignment. But it seems to
        // assume that ThreadControlBlock is allocated by `malloc` (hence 16
        // byte aligned?).
        assert!(
            tls_all_layout.size() == mem::size_of::<ThreadControlBlock>() + tls_data_layout.size(),
            "`extend` did not insert padding"
        );
        assert!(tls_data_layout.size() > 0, "allocate a non-zero size");

        let tls_base: *mut u8 = unsafe { alloc::alloc::alloc_zeroed(tls_all_layout) };
        assert_ne!(tls_base, ptr::null_mut(), "Out of memory during init?");

        unsafe {
            // Safety `add`:
            // - Start and result pointer must be in bounds or one byte past the end of the same object:
            assert!(tls_all_layout.size() >= mem::size_of::<ThreadControlBlock>());
            // - Compute offset (bytes can not overflow) isize: Yes, layout is
            //   valid, and we substract from it
            // - The offset being in bounds cannot rely on "wrapping around" the
            //   address space: Yes, trivial
            let tcb_start_addr =
                tls_base.add(tls_all_layout.size() - mem::size_of::<ThreadControlBlock>());

            // Safety deref+assignment:
            // - We know location is valid and aligned because of all the work above
            // - No mutable outside of this function
            let tcb = tcb_start_addr as *mut ThreadControlBlock;
            // - Copy of plain-old-data, raw-pointer
            *tcb = TCB_INITIAL;
            // Safety: TLS lookups on x86 will be using `fs:0x0` so we have add
            // a self-pointer as the first arg of the TCB
            (*tcb).tcb_myself = tcb;

            log::trace!(
                "initial_tdata {:?} len={}, tls_base = {:?} tcb = {:?}",
                initial_tdata,
                initial_tdata.len(),
                tls_base,
                tcb
            );

            // Initialize tdata section with template data from ELF:
            // Safety `copy_from_nonoverlapping`:
            // - src must be valid for reads of count * size_of::<T>() bytes: Yes (see allocation above)
            // - dst must be valid for writes of count * size_of::<T>() bytes: Yes (see allocation above)
            // - Both src and dst must be properly aligned: Yes (see allocation above)
            // - No overlap: Yes (assuming allocator/ELF parsing is correct)
            tls_base.copy_from_nonoverlapping(initial_tdata.as_ptr(), initial_tdata.len());

            tcb as *const ThreadControlBlock
        }
    }

    /// Installs a ThreadControBlock in the `tpidrro` register of the current core.
    ///
    /// # Safety
    ///
    /// - This will set the memory for all `#[thread_local]` variables and
    ///   should *only* be called during init when setting up TLS.
    ///
    /// - Assumes that `ptr` has been correctly allocated and initialized (with
    ///   [`ThreadControlBlock::new`]) and `ptr` is only used by one core.
    ///
    /// - The same `ptr` also needs to be restored/saved upon entering/exiting
    ///   from syscalls/interrupt. This is not necessarily a safety concern for
    ///   this function (as long as we don't change `ptr` after init (and we
    ///   shouldn't)). `fs` save/restore is handled in the assembly code for
    ///   syscalls/interrupts.
    unsafe fn install(ptr: *const ThreadControlBlock) {
        TPIDR_EL0.set(ptr as u64)
    }

    pub(crate) fn try_get_tcb<'a>() -> Option<&'a mut ThreadControlBlock> {
        unsafe {
            // Safety:
            // - TODO(safety+soundness): not safe, should return a non-mut reference
            //   with mutable stuff (it's just save_area that's left) wrapped in
            //   RefCell or similar (treat the same as a thread-local)
            let kcb_raw = TPIDR_EL0.get();
            if kcb_raw != 0 {
                Some(ThreadControlBlock::get_tcb())
            } else {
                None
            }
        }
    }

    fn get_tcb<'a>() -> &'a mut ThreadControlBlock {
        unsafe {
            let tcb_raw = TPIDR_EL0.get();
            let tcb = tcb_raw as *mut ThreadControlBlock;

            assert!(tcb != ptr::null_mut(), "TCB not found in gs register.");
            let kptr = ptr::NonNull::new_unchecked(tcb);
            &mut *kptr.as_ptr()
        }
    }
}
